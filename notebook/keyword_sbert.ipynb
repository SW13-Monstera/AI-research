{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a697ff14",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##%% md\n",
    "## !pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fd5d5d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"Huffon/sentence-klue-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff5f4d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06601b3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('user_answer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4ad0d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ff1400",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0503910",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# NAN data 제거\n",
    "print(f\"pre data size : {len(df)}\")\n",
    "df['user_answer'] = df['user_answer'].str.strip().str.replace(\"\\n\", \"\").str.replace(\"\\xa0\", \"\").str.replace(\"  \", \" \")\n",
    "df['user_answer'].replace('', np.nan, inplace=True)\n",
    "df.dropna(axis=0, subset=['user_answer'], inplace=True)  # 빈 답변 제거\n",
    "print(f\"after data size : {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1653db86",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "min_len = min(map(len, df['user_answer']))\n",
    "max_len = max(map(len, df['user_answer']))\n",
    "\n",
    "for i, data in df.iterrows():\n",
    "    if len(data['user_answer']) == min_len:\n",
    "        print(f\"가장 짧은 답변 {min_len}ch:\\n\\t {data['user_answer']}\")\n",
    "    elif len(data['user_answer']) == max_len:\n",
    "        print(f\"가장 긴 답변 {max_len}ch:\\n\\t {data['user_answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da52c83",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4203d82",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c1370",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class Problem:\n",
    "    subject: str  # 문제별\n",
    "    keywords: List[str]  # 문제별\n",
    "    keywords_score: List[int]  # 문제별\n",
    "    keywords_embedding : List[np.ndarray]  # 문제별\n",
    "    user_answers: List[str]  # 유저별\n",
    "    user_correct_keywords: List[List[str]]  # 유저별\n",
    "    ground_truths: List[List[int]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9871d61",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f2a2b7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "# criterion parsing\n",
    "for i, data in df.iterrows():\n",
    "    problem_id = data['problem_id']\n",
    "    if problem_id not in dataset:\n",
    "        keywords = []\n",
    "        keywords_score = []\n",
    "\n",
    "        for criterion in eval(data['keyword_criterion']):\n",
    "            keyword, score = map(str.strip, criterion.split('-'))\n",
    "            score = float(score.split(\"점\")[0])\n",
    "            keywords.append(keyword)\n",
    "            keywords_score.append(score)\n",
    "        keywords_embedding = model.encode(keywords)\n",
    "\n",
    "        dataset[problem_id] = Problem(\n",
    "            subject=data['problem'],\n",
    "            keywords=keywords,\n",
    "            keywords_score=keywords_score,\n",
    "            keywords_embedding=keywords_embedding,\n",
    "            user_answers=[],\n",
    "            user_correct_keywords=[],\n",
    "            ground_truths=[],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4718df4e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "problem_id = random.choice(list(dataset.keys()))\n",
    "print(f\"problem : {dataset[problem_id].subject}\")\n",
    "print(f\"keywords : {dataset[problem_id].keywords}\")\n",
    "print(f\"keywords_score : {dataset[problem_id].keywords_score}\")\n",
    "print(f\"keyword embedding shape :{dataset[problem_id].keywords_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8511c228",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i, data in df.iterrows():\n",
    "    problem = dataset[data['problem_id']]\n",
    "    user_answer = data['user_answer']\n",
    "    user_correct_keyword = [criterion.split('-')[0].rstrip() for criterion in eval(data['correct_keyword_criterion'])]\n",
    "    ground_truth = [1 if keyword in user_correct_keyword else 0 for keyword in problem.keywords]\n",
    "    problem.user_correct_keywords.append(user_correct_keyword)\n",
    "    problem.user_answers.append(user_answer)\n",
    "    problem.ground_truths.append(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e71d565",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"문제       : {problem.subject}\")\n",
    "print(f\"유저 답변   : {problem.user_answers[0]}\")\n",
    "print(f\"정답 키워드 : {problem.user_correct_keywords[0]}\")\n",
    "print(f\"후보 키워드 : {problem.keywords}\")\n",
    "print(f\"정답 라벨  : {problem.ground_truths[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b30dc2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 하나라도 출력되면 라벨링을 잘못 파싱한 것\n",
    "for problem_id in dataset:\n",
    "    problem = dataset[problem_id]\n",
    "    for gt, correct_keyword in zip(problem.ground_truths, problem.user_correct_keywords):\n",
    "        ground_truth = []\n",
    "        for i, flag in enumerate(gt):\n",
    "            if flag:\n",
    "                ground_truth.append(problem.keywords[i])\n",
    "        if not ground_truth == correct_keyword:\n",
    "            print(ground_truth, correct_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd77c6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for problem_id in dataset:\n",
    "    print(f\"{dataset[problem_id].subject}\")\n",
    "    print(f\"{len(dataset[problem_id].user_answers)}개의 유저 답변\")\n",
    "    print(f\"{len(dataset[problem_id].user_correct_keywords)}개의 유저의 답변별 키워드 라벨링\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f96564",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "problem_id = random.choice(list(dataset.keys()))\n",
    "user_answer = random.choice(dataset[problem_id].user_answers)\n",
    "tokenized_answer = [word[0] for word in okt.pos(user_answer) if word[1] == 'Noun']\n",
    "print(tokenized_answer[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be921176",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 유사도 검사\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "threshold = 0.7\n",
    "total_acc, total_f1 = 0, 0\n",
    "for i, key in enumerate(dataset):\n",
    "    acc, f1 = 0, 0\n",
    "    problem = dataset[key]\n",
    "    for j, (user_answer, ground_truth) in enumerate(tqdm(zip(problem.user_answers, problem.ground_truths))):\n",
    "        tokenized_answer = [word[0] for word in okt.pos(user_answer) if word[1] == 'Noun']  # 명사만 추출\n",
    "        tokenized_answer_embedding = model.encode(tokenized_answer)\n",
    "        similarity_scores = cosine_similarity(problem.keywords_embedding, tokenized_answer_embedding)\n",
    "        predicts = [1 if score.max() > 0.7 else 0 for idx, score in enumerate(similarity_scores)]\n",
    "        acc += accuracy_score(ground_truth, predicts)\n",
    "        f1 += f1_score(ground_truth, predicts, zero_division=0)\n",
    "    print(f\"{i}번 문제 점수 : accuracy : {acc / len(problem.user_answers)}, f1-score : {f1 / len(problem.user_answers)}\")\n",
    "    total_acc += acc / len(problem.user_answers)\n",
    "    total_f1 += f1 / len(problem.user_answers)\n",
    "print(f\"전체 문제 평균 accuracy : {total_acc / len(dataset)}, f1-score : {total_f1 / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700efb62",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "problem_id = random.choice(list(dataset.keys()))\n",
    "user_answer = random.choice(dataset[problem_id].user_answers)\n",
    "tokenized_answer = [word[0] for word in okt.pos(user_answer) if word[1] in ('Noun', 'Alpha')]\n",
    "print(tokenized_answer[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93834dc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 유사도 검사 -> 영어도 추가\n",
    "threshold = 0.7\n",
    "total_acc, total_f1 = 0, 0\n",
    "for i, key in enumerate(dataset):\n",
    "    acc, f1 = 0, 0\n",
    "    problem = dataset[key]\n",
    "    for j, (user_answer, ground_truth) in enumerate(tqdm(zip(problem.user_answers, problem.ground_truths))):\n",
    "        tokenized_answer = [word[0] for word in okt.pos(user_answer) if word[1] in ('Noun', 'Alpha')]  # 명사만 추출\n",
    "        tokenized_answer_embedding = model.encode(tokenized_answer)\n",
    "        similarity_scores = cosine_similarity(problem.keywords_embedding, tokenized_answer_embedding)\n",
    "        predicts = [1 if score.max() > 0.7 else 0 for idx, score in enumerate(similarity_scores)]\n",
    "        acc += accuracy_score(ground_truth, predicts)\n",
    "        f1 += f1_score(ground_truth, predicts, zero_division=0)\n",
    "    print(f\"{i}번 문제 점수 : accuracy : {acc / len(problem.user_answers)}, f1-score : {f1 / len(problem.user_answers)}\")\n",
    "    total_acc += acc / len(problem.user_answers)\n",
    "    total_f1 += f1 / len(problem.user_answers)\n",
    "print(f\"전체 문제 평균 accuracy : {total_acc / len(dataset)}, f1-score : {total_f1 / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541405e9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "### 2번이랑 3번이랑 아니 그냥 전체적으로 엉망진창이네!!\n",
    "### 성능 향상을 위한 방법들은 아래와 같다.\n",
    "- Tokenizing을 하지 않고 n-gram 방식으로 대조\n",
    "- 키워드를 조금 더 적합한 단어로 변경\n",
    "- 여러개의 키워드 후보군을 비교\n",
    "- 불용어 제거\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b62106b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i, problem_id in enumerate(dataset):\n",
    "    problem = dataset[problem_id]\n",
    "    print(f\"{i}번째 문제 ID : {problem_id}\")\n",
    "    print(f\"{i}번째 문제 : {problem.subject}\")\n",
    "    print(f\"keyword : {problem.keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4166b372",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(dataset['recXfKthnQLwWETgb'].keywords)\n",
    "dataset['recXfKthnQLwWETgb'].keywords[0] = \"만료\"  # Lifecycle -> 만료\n",
    "print(dataset['recXfKthnQLwWETgb'].keywords)\n",
    "dataset['recXfKthnQLwWETgb'].keywords_embedding = model.encode(dataset['recXfKthnQLwWETgb'].keywords)  # 1번\n",
    "print(dataset['rec1QvAvB4CMami3p'].keywords)\n",
    "dataset['rec1QvAvB4CMami3p'].keywords[2] = '스택'  # Stack -> 스택\n",
    "print(dataset['rec1QvAvB4CMami3p'].keywords)\n",
    "dataset['rec1QvAvB4CMami3p'].keywords_embedding = model.encode(dataset['rec1QvAvB4CMami3p'].keywords)  # 7번\n",
    "print(dataset['recUcGjT9Xkb7N5pu'].keywords)\n",
    "dataset['recUcGjT9Xkb7N5pu'].keywords[2] = \"삭제\"  # 삭제(POP) -> 삭제\n",
    "dataset['recUcGjT9Xkb7N5pu'].keywords[3] = \"삽입\"  # 삽입(PUSH) -> 삽입\n",
    "print(dataset['recUcGjT9Xkb7N5pu'].keywords)\n",
    "dataset['recUcGjT9Xkb7N5pu'].keywords_embedding = model.encode(dataset['recUcGjT9Xkb7N5pu'].keywords)  # 10번"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b24d29",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "problem_id = random.choice(list(dataset.keys()))\n",
    "split_answer = random.choice(dataset[problem_id].user_answers).split(' ')\n",
    "word_concat_size = 2\n",
    "tokenized_answer = []\n",
    "for k in range(len(split_answer) - word_concat_size):\n",
    "    tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "print(tokenized_answer[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fee420",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 유사도 검사 -> Tokenizer 방식에서 띄어쓰기 단위로 2개씩 묶어서 window 방식으로 비교 + 키워드 변형\n",
    "threshold = 0.7\n",
    "total_acc, total_f1 = 0, 0\n",
    "word_concat_size = 2\n",
    "\n",
    "for i, key in enumerate(dataset):\n",
    "    acc, f1 = 0, 0\n",
    "    problem = dataset[key]\n",
    "    for j, (user_answer, ground_truth) in enumerate(tqdm(zip(problem.user_answers, problem.ground_truths))):\n",
    "        split_answer = user_answer.split(' ')\n",
    "        tokenized_answer = []\n",
    "        for k in range(len(split_answer) - word_concat_size):\n",
    "            tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "        tokenized_answer_embedding = model.encode(tokenized_answer)\n",
    "        similarity_scores = cosine_similarity(problem.keywords_embedding, tokenized_answer_embedding)\n",
    "        predicts = [1 if score.max() > threshold else 0 for idx, score in enumerate(similarity_scores)]\n",
    "        acc += accuracy_score(ground_truth, predicts)\n",
    "        f1 += f1_score(ground_truth, predicts, zero_division=0)\n",
    "    print(f\"{i}번 문제 점수 : accuracy : {acc / len(problem.user_answers)}, f1-score : {f1 / len(problem.user_answers)}\")\n",
    "    total_acc += acc / len(problem.user_answers)\n",
    "    total_f1 += f1 / len(problem.user_answers)\n",
    "print(f\"전체 문제 평균 accuracy : {total_acc / len(dataset)}, f1-score : {total_f1 / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abea29e5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 유사도 검사 -> Tokenizer 방식에서 띄어쓰기 단위로 2개씩 묶어서 window 방식으로 비교 + 키워드 변형, threshold 0.5로 낮춤\n",
    "threshold = 0.5\n",
    "total_acc, total_f1 = 0, 0\n",
    "word_concat_size = 2\n",
    "\n",
    "for i, key in enumerate(dataset):\n",
    "    acc, f1 = 0, 0\n",
    "    problem = dataset[key]\n",
    "    for j, (user_answer, ground_truth) in enumerate(tqdm(zip(problem.user_answers, problem.ground_truths))):\n",
    "        split_answer = user_answer.split(' ')\n",
    "        tokenized_answer = []\n",
    "        for k in range(len(split_answer) - word_concat_size):\n",
    "            tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "        tokenized_answer_embedding = model.encode(tokenized_answer)\n",
    "        similarity_scores = cosine_similarity(problem.keywords_embedding, tokenized_answer_embedding)\n",
    "        predicts = [1 if score.max() > threshold else 0 for idx, score in enumerate(similarity_scores)]\n",
    "        acc += accuracy_score(ground_truth, predicts)\n",
    "        f1 += f1_score(ground_truth, predicts, zero_division=0)\n",
    "    print(f\"{i}번 문제 점수 : accuracy : {acc / len(problem.user_answers)}, f1-score : {f1 / len(problem.user_answers)}\")\n",
    "    total_acc += acc / len(problem.user_answers)\n",
    "    total_f1 += f1 / len(problem.user_answers)\n",
    "print(f\"전체 문제 평균 accuracy : {total_acc / len(dataset)}, f1-score : {total_f1 / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c69934",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "problem_id = random.choice(list(dataset.keys()))\n",
    "split_answer = random.choice(dataset[problem_id].user_answers).split(' ')\n",
    "word_concat_size = 3\n",
    "tokenized_answer = []\n",
    "for k in range(len(split_answer) - word_concat_size):\n",
    "    tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "print(tokenized_answer[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d612042a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 유사도 검사 -> Tokenizer 방식에서 띄어쓰기 단위로 2개씩 묶어서 window 방식으로 비교 + 키워드 변형 -> 윈도우 사이즈 3으로 늘림, threshold 0.35로 낮춤\n",
    "threshold = 0.35\n",
    "total_acc, total_f1 = 0, 0\n",
    "word_concat_size = 3\n",
    "\n",
    "for i, key in enumerate(dataset):\n",
    "    acc, f1 = 0, 0\n",
    "    problem = dataset[key]\n",
    "    for j, (user_answer, ground_truth) in enumerate(tqdm(zip(problem.user_answers, problem.ground_truths))):\n",
    "        split_answer = user_answer.split(' ')\n",
    "        tokenized_answer = []\n",
    "        for k in range(len(split_answer) - word_concat_size + 1):\n",
    "            tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "        if len(split_answer) < word_concat_size:\n",
    "            tokenized_answer.append(' '.join(split_answer))\n",
    "        tokenized_answer_embedding = model.encode(tokenized_answer)\n",
    "        similarity_scores = cosine_similarity(problem.keywords_embedding, tokenized_answer_embedding)\n",
    "        predicts = [1 if score.max() > threshold else 0 for idx, score in enumerate(similarity_scores)]\n",
    "\n",
    "        acc += accuracy_score(ground_truth, predicts)\n",
    "        f1 += f1_score(ground_truth, predicts, zero_division=1)\n",
    "    print(f\"{i}번 문제 점수 : accuracy : {acc / len(problem.user_answers)}, f1-score : {f1 / len(problem.user_answers)}\")\n",
    "    total_acc += acc / len(problem.user_answers)\n",
    "    total_f1 += f1 / len(problem.user_answers)\n",
    "print(f\"전체 문제 평균 accuracy : {total_acc / len(dataset)}, f1-score : {total_f1 / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72409213",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "### 왜 5번문제만 f1 score가 저 모양일까?\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebddccb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "problem = dataset['recio3s0A77i0kkEn']\n",
    "\n",
    "print(problem.subject)\n",
    "print(f\"정답 키워드 : {problem.keywords}\")\n",
    "keyword_count = {}\n",
    "for answer, keywords in zip(problem.user_answers, problem.user_correct_keywords):\n",
    "    for keyword in keywords:\n",
    "        if keyword not in keyword_count:\n",
    "            keyword_count[keyword] = 1\n",
    "        else:\n",
    "            keyword_count[keyword] += 1\n",
    "for keyword in keyword_count:\n",
    "    keyword_count[keyword] = f\"{int(keyword_count[keyword] / len(problem.user_answers) * 100)}%\"\n",
    "print(keyword_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0884aa8b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "#### 자세히 보면 키워드가 너무 어려웠는지 정답률이 엉망이다.\n",
    "#### 실제 정답에 1이 없다면 f1 score는 무조건 0이다.\n",
    "#### 이는 키워드가 적절치 않았는지를 고려해봐야 할 듯 하다.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e08ca3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "### 이제 실제로 모델이 어떻게 예측하고 있는지 눈으로 확인해보자!!\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03fcf98",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 유사도 검사 -> Tokenizer 방식에서 띄어쓰기 단위로 2개씩 묶어서 window 방식으로 비교 + 키워드 변형 -> 윈도우 사이즈 3으로 늘림\n",
    "threshold = 0.35\n",
    "total_acc, total_f1 = 0, 0\n",
    "word_concat_size = 3\n",
    "\n",
    "for i, key in enumerate(dataset):\n",
    "    acc, f1 = 0, 0\n",
    "    problem = dataset[key]\n",
    "    for j, (user_answer, ground_truth) in enumerate(tqdm(zip(problem.user_answers, problem.ground_truths))):\n",
    "        if j == 10:\n",
    "            break\n",
    "        split_answer = user_answer.split(' ')\n",
    "        tokenized_answer = []\n",
    "        for k in range(len(split_answer) - word_concat_size + 1):\n",
    "            tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "        if len(split_answer) < word_concat_size:\n",
    "            tokenized_answer.append(' '.join(split_answer))\n",
    "        tokenized_answer_embedding = model.encode(tokenized_answer)\n",
    "        similarity_scores = cosine_similarity(problem.keywords_embedding, tokenized_answer_embedding)\n",
    "        predicts = []\n",
    "        for z, idx in enumerate(similarity_scores.argmax(axis=1)):\n",
    "            if threshold < similarity_scores[z][idx]:\n",
    "                print(f\"keyword          : {problem.keywords[z]}\")\n",
    "                print(f\"detected keyword : {tokenized_answer[idx]}\")\n",
    "                predicts.append(1)\n",
    "            else:\n",
    "                predicts.append(0)\n",
    "        print(f\"실제 정답 : {problem.user_correct_keywords[j]}\")\n",
    "        acc += accuracy_score(ground_truth, predicts)\n",
    "        f1 += f1_score(ground_truth, predicts, zero_division=1)\n",
    "    print(f\"{i}번 문제 점수 : accuracy : {acc / len(problem.user_answers)}, f1-score : {f1 / len(problem.user_answers)}\")\n",
    "    total_acc += acc / len(problem.user_answers)\n",
    "    total_f1 += f1 / len(problem.user_answers)\n",
    "print(f\"전체 문제 평균 accuracy : {total_acc / len(dataset)}, f1-score : {total_f1 / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515cd524",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "### 처음에 tokenized 되면서 영어들이 무시되었기 때문에 영어로 구성된 키워드들을 한글로 변형 시켰지만 스윽 보니 이제 영어도 잘 찾네요!!\n",
    "### 그럼 다시 이전 키워드들로 변경해서 성능을 비교해봅시다\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a0bd6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(dataset['recXfKthnQLwWETgb'].keywords)\n",
    "dataset['recXfKthnQLwWETgb'].keywords[0] = \"Lifecycle\"  # 만료 -> Lifecycle\n",
    "print(dataset['recXfKthnQLwWETgb'].keywords)\n",
    "dataset['recXfKthnQLwWETgb'].keywords_embedding = model.encode(dataset['recXfKthnQLwWETgb'].keywords)  # 1번\n",
    "print(dataset['recUcGjT9Xkb7N5pu'].keywords)\n",
    "dataset['recUcGjT9Xkb7N5pu'].keywords[2] = \"삭제(POP)\"  # 삭제 -> 삭제(POP)\n",
    "dataset['recUcGjT9Xkb7N5pu'].keywords[3] = \"삽입(PUSH)\"  # 삽입 -> 삽입(PUSH)\n",
    "print(dataset['recUcGjT9Xkb7N5pu'].keywords)\n",
    "dataset['recUcGjT9Xkb7N5pu'].keywords_embedding = model.encode(dataset['recUcGjT9Xkb7N5pu'].keywords)  # 10번\n",
    "print(dataset['rec1QvAvB4CMami3p'].keywords)\n",
    "dataset['rec1QvAvB4CMami3p'].keywords[2] = 'Stack'  # 스택 -> Stack\n",
    "dataset['rec1QvAvB4CMami3p'].keywords_embedding = model.encode(dataset['rec1QvAvB4CMami3p'].keywords)  # 7번\n",
    "print(dataset['rec1QvAvB4CMami3p'].keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e6b36",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 유사도 검사 -> Tokenizer 방식에서 띄어쓰기 단위로 3개씩 묶어서 window 방식으로 비교 + 키워드 변형 -> 윈도우 사이즈 3으로 늘림\n",
    "threshold = 0.35\n",
    "total_acc, total_f1 = 0, 0\n",
    "word_concat_size = 3\n",
    "\n",
    "for i, key in enumerate(dataset):\n",
    "    acc, f1 = 0, 0\n",
    "    problem = dataset[key]\n",
    "    for j, (user_answer, ground_truth) in enumerate(tqdm(zip(problem.user_answers, problem.ground_truths))):\n",
    "        split_answer = user_answer.split(' ')\n",
    "        tokenized_answer = []\n",
    "        for k in range(len(split_answer) - word_concat_size + 1):\n",
    "            tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "        if len(split_answer) < word_concat_size:\n",
    "            tokenized_answer.append(' '.join(split_answer))\n",
    "        tokenized_answer_embedding = model.encode(tokenized_answer)\n",
    "        similarity_scores = cosine_similarity(problem.keywords_embedding, tokenized_answer_embedding)\n",
    "        predicts = [1 if score.max() > threshold else 0 for idx, score in enumerate(similarity_scores)]\n",
    "\n",
    "        acc += accuracy_score(ground_truth, predicts)\n",
    "        f1 += f1_score(ground_truth, predicts, zero_division=1)\n",
    "    print(f\"{i}번 문제 점수 : accuracy : {acc / len(problem.user_answers)}, f1-score : {f1 / len(problem.user_answers)}\")\n",
    "    total_acc += acc / len(problem.user_answers)\n",
    "    total_f1 += f1 / len(problem.user_answers)\n",
    "print(f\"전체 문제 평균 accuracy : {total_acc / len(dataset)}, f1-score : {total_f1 / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f551a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "### 1번 문제 : 만료 -> Lifecycle [성능 소폭 하락]\n",
    "### 7번 문제 : 삽입 -> 스택 -> Stack [성능 상승]\n",
    "### 10번 문제 : 삭제 -> 삭제(POP), 삽입 -> 삽입(PUSH) [성능 하락]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d493e927",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "### 이번에는 여러개의 키워드라면 쉼표로 구분해서 넣어줘보자\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8948e98f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(dataset['recXfKthnQLwWETgb'].keywords)\n",
    "dataset['recXfKthnQLwWETgb'].keywords[0] = \"만료, Lifecycle\"\n",
    "print(dataset['recXfKthnQLwWETgb'].keywords)\n",
    "dataset['recXfKthnQLwWETgb'].keywords_embedding = model.encode(dataset['recXfKthnQLwWETgb'].keywords)  # 1번\n",
    "print(dataset['recUcGjT9Xkb7N5pu'].keywords)\n",
    "dataset['recUcGjT9Xkb7N5pu'].keywords[2] = \"삭제, POP\"  # 삭제 -> 삭제(POP)\n",
    "dataset['recUcGjT9Xkb7N5pu'].keywords[3] = \"삽입, PUSH\"  # 삽입 -> 삽입(PUSH)\n",
    "print(dataset['recUcGjT9Xkb7N5pu'].keywords)\n",
    "dataset['recUcGjT9Xkb7N5pu'].keywords_embedding = model.encode(dataset['recUcGjT9Xkb7N5pu'].keywords)  # 10번\n",
    "print(dataset['rec1QvAvB4CMami3p'].keywords)\n",
    "dataset['rec1QvAvB4CMami3p'].keywords[2] = 'Stack'  # 스택 -> Stack\n",
    "dataset['rec1QvAvB4CMami3p'].keywords_embedding = model.encode(dataset['rec1QvAvB4CMami3p'].keywords)  # 7번\n",
    "print(dataset['rec1QvAvB4CMami3p'].keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b81dc08",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 유사도 검사 -> Tokenizer 방식에서 띄어쓰기 단위로 2개씩 묶어서 window 방식으로 비교 + 키워드 변형 -> 윈도우 사이즈 3으로 늘림\n",
    "threshold = 0.35\n",
    "total_acc, total_f1 = 0, 0\n",
    "word_concat_size = 3\n",
    "\n",
    "for i, key in enumerate(dataset):\n",
    "    acc, f1 = 0, 0\n",
    "    problem = dataset[key]\n",
    "    for j, (user_answer, ground_truth) in enumerate(tqdm(zip(problem.user_answers, problem.ground_truths))):\n",
    "        split_answer = user_answer.split(' ')\n",
    "        tokenized_answer = []\n",
    "        for k in range(len(split_answer) - word_concat_size + 1):\n",
    "            tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "        if len(split_answer) < word_concat_size:\n",
    "            tokenized_answer.append(' '.join(split_answer))\n",
    "        tokenized_answer_embedding = model.encode(tokenized_answer)\n",
    "        similarity_scores = cosine_similarity(problem.keywords_embedding, tokenized_answer_embedding)\n",
    "        predicts = [1 if score.max() > threshold else 0 for idx, score in enumerate(similarity_scores)]\n",
    "\n",
    "        acc += accuracy_score(ground_truth, predicts)\n",
    "        f1 += f1_score(ground_truth, predicts, zero_division=1)\n",
    "    print(f\"{i}번 문제 점수 : accuracy : {acc / len(problem.user_answers)}, f1-score : {f1 / len(problem.user_answers)}\")\n",
    "    total_acc += acc / len(problem.user_answers)\n",
    "    total_f1 += f1 / len(problem.user_answers)\n",
    "print(f\"전체 문제 평균 accuracy : {total_acc / len(dataset)}, f1-score : {total_f1 / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f2e36",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "### 꽤 유의미한 결과다!!\n",
    "### 마지막으로 word concat size를 2로 줄여서 두개의 단어씩만 비교해보자\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b529166f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "problem_id = random.choice(list(dataset.keys()))\n",
    "\n",
    "split_answer = random.choice(dataset[problem_id].user_answers).split(' ')\n",
    "word_concat_size = 2\n",
    "tokenized_answer = []\n",
    "for k in range(len(split_answer) - word_concat_size):\n",
    "    tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "print(tokenized_answer[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a997d7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 유사도 검사 -> Tokenizer 방식에서 띄어쓰기 단위로 2개씩 묶어서 window 방식으로 비교 + 키워드 변형 -> 윈도우 사이즈 3으로 늘림\n",
    "threshold = 0.35\n",
    "total_acc, total_f1 = 0, 0\n",
    "word_concat_size = 2\n",
    "\n",
    "for i, key in enumerate(dataset):\n",
    "    acc, f1 = 0, 0\n",
    "    problem = dataset[key]\n",
    "    for j, (user_answer, ground_truth) in enumerate(tqdm(zip(problem.user_answers, problem.ground_truths))):\n",
    "        split_answer = user_answer.split(' ')\n",
    "        tokenized_answer = []\n",
    "        for k in range(len(split_answer) - word_concat_size + 1):\n",
    "            tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "        if len(split_answer) < word_concat_size:\n",
    "            tokenized_answer.append(' '.join(split_answer))\n",
    "        tokenized_answer_embedding = model.encode(tokenized_answer)\n",
    "        similarity_scores = cosine_similarity(problem.keywords_embedding, tokenized_answer_embedding)\n",
    "        predicts = [1 if score.max() > threshold else 0 for idx, score in enumerate(similarity_scores)]\n",
    "\n",
    "        acc += accuracy_score(ground_truth, predicts)\n",
    "        f1 += f1_score(ground_truth, predicts, zero_division=1)\n",
    "    print(f\"{i}번 문제 점수 : accuracy : {acc / len(problem.user_answers)}, f1-score : {f1 / len(problem.user_answers)}\")\n",
    "    total_acc += acc / len(problem.user_answers)\n",
    "    total_f1 += f1 / len(problem.user_answers)\n",
    "print(f\"전체 문제 평균 accuracy : {total_acc / len(dataset)}, f1-score : {total_f1 / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22274f39",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "### 전체 정확도는 조금 떨어졌지만 분명 2개씩 봤을 때 성능이 더 좋은 문제들이 있다.\n",
    "### 확실하게 하려면 다음부터는 2개씩 보는 것과 3개씩 보는 것, 그리고 쉼표로 여러 키워드 후보를 비교해보는게 아닌 각각 여러개 비교하기까지 하면 성능이 꽤나 좋아질 것 같다.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5953d1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 마지막으로 윈도우 사이즈를 2개와 3개를 합쳐서 해보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a57531",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "problem_id = random.choice(list(dataset.keys()))\n",
    "split_answer = random.choice(dataset[problem_id].user_answers).split(' ')\n",
    "word_concat_size = 2\n",
    "for k in range(len(split_answer) - word_concat_size + 1):\n",
    "    tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "word_concat_size = 3\n",
    "for k in range(len(split_answer) - word_concat_size + 1):\n",
    "    tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "print(tokenized_answer[:3])\n",
    "print(tokenized_answer[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f7f13",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 유사도 검사 -> Tokenizer 방식에서 띄어쓰기 단위로 2개씩 묶어서 window 방식으로 비교 + 키워드 변형 -> 윈도우 사이즈 3으로 늘림\n",
    "threshold = 0.35\n",
    "total_acc, total_f1 = 0, 0\n",
    "\n",
    "for i, key in enumerate(dataset):\n",
    "    acc, f1 = 0, 0\n",
    "    problem = dataset[key]\n",
    "    for j, (user_answer, ground_truth) in enumerate(tqdm(zip(problem.user_answers, problem.ground_truths))):\n",
    "        split_answer = user_answer.split(' ')\n",
    "        tokenized_answer = []\n",
    "        word_concat_size = 2\n",
    "        for k in range(len(split_answer) - word_concat_size + 1):\n",
    "            tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "        word_concat_size = 3\n",
    "        for k in range(len(split_answer) - word_concat_size + 1):\n",
    "            tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "        if not tokenized_answer:\n",
    "            tokenized_answer.append(' '.join(split_answer))\n",
    "        tokenized_answer_embedding = model.encode(tokenized_answer)\n",
    "        similarity_scores = cosine_similarity(problem.keywords_embedding, tokenized_answer_embedding)\n",
    "        predicts = [1 if score.max() > threshold else 0 for idx, score in enumerate(similarity_scores)]\n",
    "\n",
    "        acc += accuracy_score(ground_truth, predicts)\n",
    "        f1 += f1_score(ground_truth, predicts, zero_division=1)\n",
    "    print(f\"{i}번 문제 점수 : accuracy : {acc / len(problem.user_answers)}, f1-score : {f1 / len(problem.user_answers)}\")\n",
    "    total_acc += acc / len(problem.user_answers)\n",
    "    total_f1 += f1 / len(problem.user_answers)\n",
    "print(f\"전체 문제 평균 accuracy : {total_acc / len(dataset)}, f1-score : {total_f1 / len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ceea38",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 하지만 이런 방식은 너무 느리다. GPU를 사용한다해도 이는 감당이 안될 수 있다.\n",
    "### 그리고 정확도도 그리 좋아지지 않았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a243db3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
