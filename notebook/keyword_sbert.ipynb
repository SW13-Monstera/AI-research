{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a697ff14",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##%% md\n",
    "## !pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42fd5d5d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /Users/minjaewon/.cache/torch/sentence_transformers/Huffon_sentence-klue-roberta-base. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"Huffon/sentence-klue-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.backends.mps.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mps_device = torch.device(\"mps\")\n",
    "model.to(mps_device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./../static/labeled_dataset.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# NAN data 제거\n",
    "print(f\"pre data size : {len(df)}\")\n",
    "df['user_answer'] = df['user_answer'].str.strip().str.replace(\"\\n\", \"\").str.replace(\"\\xa0\", \"\").str.replace(\"  \", \" \")\n",
    "df['user_answer'].replace('', np.nan, inplace=True)\n",
    "df.dropna(axis=0, subset=['user_answer'], inplace=True)  # 빈 답변 제거\n",
    "print(f\"after data size : {len(df)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "min_len = min(map(len, df['user_answer']))\n",
    "max_len = max(map(len, df['user_answer']))\n",
    "\n",
    "for i, data in df.iterrows():\n",
    "    if len(data['user_answer']) == min_len:\n",
    "        print(f\"가장 짧은 답변 {min_len}ch:\\n\\t {data['user_answer']}\")\n",
    "    elif len(data['user_answer']) == max_len:\n",
    "        print(f\"가장 긴 답변 {max_len}ch:\\n\\t {data['user_answer']}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### pip install konlpy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class Problem:\n",
    "    subject: str  # 문제별\n",
    "    keywords: List[str]  # 문제별\n",
    "    keywords_score: List[int]  # 문제별\n",
    "    keywords_embedding : List[np.ndarray]  # 문제별\n",
    "    user_answers: List[str]  # 유저별\n",
    "    user_correct_keywords: List[List[str]]  # 유저별\n",
    "    ground_truths: List[List[int]]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "# criterion parsing\n",
    "for i, data in df.iterrows():\n",
    "    problem_id = data['problem_id']\n",
    "    if problem_id not in dataset:\n",
    "        keywords = []\n",
    "        keywords_score = []\n",
    "\n",
    "        for criterion in eval(data['keyword_criterion']):\n",
    "            keyword, score = map(str.strip, criterion.split('-'))\n",
    "            score = float(score.split(\"점\")[0])\n",
    "            keywords.append(keyword)\n",
    "            keywords_score.append(score)\n",
    "        keywords_embedding = model.encode(keywords)\n",
    "\n",
    "        dataset[problem_id] = Problem(\n",
    "            subject=data['problem'],\n",
    "            keywords=keywords,\n",
    "            keywords_score=keywords_score,\n",
    "            keywords_embedding=keywords_embedding,\n",
    "            user_answers=[],\n",
    "            user_correct_keywords=[],\n",
    "            ground_truths=[],\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "problem_id = random.choice(list(dataset.keys()))\n",
    "print(f\"problem : {dataset[problem_id].subject}\")\n",
    "print(f\"keywords : {dataset[problem_id].keywords}\")\n",
    "print(f\"keywords_score : {dataset[problem_id].keywords_score}\")\n",
    "print(f\"keyword embedding shape :{dataset[problem_id].keywords_embedding.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i, data in df.iterrows():\n",
    "    problem = dataset[data['problem_id']]\n",
    "    user_answer = data['user_answer']\n",
    "    user_correct_keyword = [criterion.split('-')[0].rstrip() for criterion in eval(data['correct_keyword_criterion'])]\n",
    "    ground_truth = [1 if keyword in user_correct_keyword else 0 for keyword in problem.keywords]\n",
    "    problem.user_correct_keywords.append(user_correct_keyword)\n",
    "    problem.user_answers.append(user_answer)\n",
    "    problem.ground_truths.append(ground_truth)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"문제       : {problem.subject}\")\n",
    "print(f\"유저 답변   : {problem.user_answers[0]}\")\n",
    "print(f\"정답 키워드 : {problem.user_correct_keywords[0]}\")\n",
    "print(f\"후보 키워드 : {problem.keywords}\")\n",
    "print(f\"정답 라벨  : {problem.ground_truths[0]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 하나라도 출력되면 라벨링을 잘못 파싱한 것\n",
    "for problem_id in dataset:\n",
    "    problem = dataset[problem_id]\n",
    "    for gt, correct_keyword in zip(problem.ground_truths, problem.user_correct_keywords):\n",
    "        ground_truth = []\n",
    "        for i, flag in enumerate(gt):\n",
    "            if flag:\n",
    "                ground_truth.append(problem.keywords[i])\n",
    "        if not ground_truth == correct_keyword:\n",
    "            print(ground_truth, correct_keyword)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for problem_id in dataset:\n",
    "    print(f\"{dataset[problem_id].subject}\")\n",
    "    print(f\"{len(dataset[problem_id].user_answers)}개의 유저 답변\")\n",
    "    print(f\"{len(dataset[problem_id].user_correct_keywords)}개의 유저의 답변별 키워드 라벨링\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "problem_id = random.choice(list(dataset.keys()))\n",
    "user_answer = random.choice(dataset[problem_id].user_answers)\n",
    "tokenized_answer = [word[0] for word in okt.pos(user_answer) if word[1] == 'Noun']\n",
    "print(tokenized_answer[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 유사도 검사\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "threshold = 0.7\n",
    "total_acc, total_f1 = 0, 0\n",
    "for i, key in enumerate(dataset):\n",
    "    acc, f1 = 0, 0\n",
    "    problem = dataset[key]\n",
    "    for j, (user_answer, ground_truth) in enumerate(tqdm(zip(problem.user_answers, problem.ground_truths))):\n",
    "        tokenized_answer = [word[0] for word in okt.pos(user_answer) if word[1] == 'Noun']  # 명사만 추출\n",
    "        tokenized_answer_embedding = model.encode(tokenized_answer)\n",
    "        similarity_scores = cosine_similarity(problem.keywords_embedding, tokenized_answer_embedding)\n",
    "        predicts = [1 if score.max() > 0.7 else 0 for idx, score in enumerate(similarity_scores)]\n",
    "        acc += accuracy_score(ground_truth, predicts)\n",
    "        f1 += f1_score(ground_truth, predicts, zero_division=0)\n",
    "    print(f\"{i}번 문제 점수 : accuracy : {acc / len(problem.user_answers)}, f1-score : {f1 / len(problem.user_answers)}\")\n",
    "    total_acc += acc / len(problem.user_answers)\n",
    "    total_f1 += f1 / len(problem.user_answers)\n",
    "print(f\"전체 문제 평균 accuracy : {total_acc / len(dataset)}, f1-score : {total_f1 / len(dataset)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "problem_id = random.choice(list(dataset.keys()))\n",
    "user_answer = random.choice(dataset[problem_id].user_answers)\n",
    "tokenized_answer = [word[0] for word in okt.pos(user_answer) if word[1] in ('Noun', 'Alpha')]\n",
    "print(tokenized_answer[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 유사도 검사 -> 영어도 추가\n",
    "threshold = 0.7\n",
    "total_acc, total_f1 = 0, 0\n",
    "for i, key in enumerate(dataset):\n",
    "    acc, f1 = 0, 0\n",
    "    problem = dataset[key]\n",
    "    for j, (user_answer, ground_truth) in enumerate(tqdm(zip(problem.user_answers, problem.ground_truths))):\n",
    "        tokenized_answer = [word[0] for word in okt.pos(user_answer) if word[1] in ('Noun', 'Alpha')]  # 명사만 추출\n",
    "        tokenized_answer_embedding = model.encode(tokenized_answer)\n",
    "        similarity_scores = cosine_similarity(problem.keywords_embedding, tokenized_answer_embedding)\n",
    "        predicts = [1 if score.max() > 0.7 else 0 for idx, score in enumerate(similarity_scores)]\n",
    "        acc += accuracy_score(ground_truth, predicts)\n",
    "        f1 += f1_score(ground_truth, predicts, zero_division=0)\n",
    "    print(f\"{i}번 문제 점수 : accuracy : {acc / len(problem.user_answers)}, f1-score : {f1 / len(problem.user_answers)}\")\n",
    "    total_acc += acc / len(problem.user_answers)\n",
    "    total_f1 += f1 / len(problem.user_answers)\n",
    "print(f\"전체 문제 평균 accuracy : {total_acc / len(dataset)}, f1-score : {total_f1 / len(dataset)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 2번이랑 3번이랑 아니 그냥 전체적으로 엉망진창이네!!\n",
    "### 성능 향상을 위한 방법들은 아래와 같다.\n",
    "- Tokenizing을 하지 않고 n-gram 방식으로 대조\n",
    "- 키워드를 조금 더 적합한 단어로 변경\n",
    "- 여러개의 키워드 후보군을 비교\n",
    "- 불용어 제거\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i, problem_id in enumerate(dataset):\n",
    "    problem = dataset[problem_id]\n",
    "    print(f\"{i}번째 문제 ID : {problem_id}\")\n",
    "    print(f\"{i}번째 문제 : {problem.subject}\")\n",
    "    print(f\"keyword : {problem.keywords}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(dataset['recXfKthnQLwWETgb'].keywords)\n",
    "dataset['recXfKthnQLwWETgb'].keywords[0] = \"만료\"  # Lifecycle -> 만료\n",
    "print(dataset['recXfKthnQLwWETgb'].keywords)\n",
    "dataset['recXfKthnQLwWETgb'].keywords_embedding = model.encode(dataset['recXfKthnQLwWETgb'].keywords)  # 1번\n",
    "print(dataset['rec1QvAvB4CMami3p'].keywords)\n",
    "dataset['rec1QvAvB4CMami3p'].keywords[2] = '스택'  # Stack -> 스택\n",
    "print(dataset['rec1QvAvB4CMami3p'].keywords)\n",
    "dataset['rec1QvAvB4CMami3p'].keywords_embedding = model.encode(dataset['rec1QvAvB4CMami3p'].keywords)  # 7번\n",
    "print(dataset['recUcGjT9Xkb7N5pu'].keywords)\n",
    "dataset['recUcGjT9Xkb7N5pu'].keywords[2] = \"삭제\"  # 삭제(POP) -> 삭제\n",
    "dataset['recUcGjT9Xkb7N5pu'].keywords[3] = \"삽입\"  # 삽입(PUSH) -> 삽입\n",
    "print(dataset['recUcGjT9Xkb7N5pu'].keywords)\n",
    "dataset['recUcGjT9Xkb7N5pu'].keywords_embedding = model.encode(dataset['recUcGjT9Xkb7N5pu'].keywords)  # 10번"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "problem_id = random.choice(list(dataset.keys()))\n",
    "split_answer = random.choice(dataset[problem_id].user_answers).split(' ')\n",
    "word_concat_size = 2\n",
    "tokenized_answer = []\n",
    "for k in range(len(split_answer) - word_concat_size):\n",
    "    tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "print(tokenized_answer[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 유사도 검사 -> Tokenizer 방식에서 띄어쓰기 단위로 2개씩 묶어서 window 방식으로 비교 + 키워드 변형\n",
    "threshold = 0.7\n",
    "total_acc, total_f1 = 0, 0\n",
    "word_concat_size = 2\n",
    "\n",
    "for i, key in enumerate(dataset):\n",
    "    acc, f1 = 0, 0\n",
    "    problem = dataset[key]\n",
    "    for j, (user_answer, ground_truth) in enumerate(tqdm(zip(problem.user_answers, problem.ground_truths))):\n",
    "        split_answer = user_answer.split(' ')\n",
    "        tokenized_answer = []\n",
    "        for k in range(len(split_answer) - word_concat_size):\n",
    "            tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "        tokenized_answer_embedding = model.encode(tokenized_answer)\n",
    "        similarity_scores = cosine_similarity(problem.keywords_embedding, tokenized_answer_embedding)\n",
    "        predicts = [1 if score.max() > threshold else 0 for idx, score in enumerate(similarity_scores)]\n",
    "        acc += accuracy_score(ground_truth, predicts)\n",
    "        f1 += f1_score(ground_truth, predicts, zero_division=0)\n",
    "    print(f\"{i}번 문제 점수 : accuracy : {acc / len(problem.user_answers)}, f1-score : {f1 / len(problem.user_answers)}\")\n",
    "    total_acc += acc / len(problem.user_answers)\n",
    "    total_f1 += f1 / len(problem.user_answers)\n",
    "print(f\"전체 문제 평균 accuracy : {total_acc / len(dataset)}, f1-score : {total_f1 / len(dataset)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 유사도 검사 -> Tokenizer 방식에서 띄어쓰기 단위로 2개씩 묶어서 window 방식으로 비교 + 키워드 변형, threshold 0.5로 낮춤\n",
    "threshold = 0.5\n",
    "total_acc, total_f1 = 0, 0\n",
    "word_concat_size = 2\n",
    "\n",
    "for i, key in enumerate(dataset):\n",
    "    acc, f1 = 0, 0\n",
    "    problem = dataset[key]\n",
    "    for j, (user_answer, ground_truth) in enumerate(tqdm(zip(problem.user_answers, problem.ground_truths))):\n",
    "        split_answer = user_answer.split(' ')\n",
    "        tokenized_answer = []\n",
    "        for k in range(len(split_answer) - word_concat_size):\n",
    "            tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "        tokenized_answer_embedding = model.encode(tokenized_answer)\n",
    "        similarity_scores = cosine_similarity(problem.keywords_embedding, tokenized_answer_embedding)\n",
    "        predicts = [1 if score.max() > threshold else 0 for idx, score in enumerate(similarity_scores)]\n",
    "        acc += accuracy_score(ground_truth, predicts)\n",
    "        f1 += f1_score(ground_truth, predicts, zero_division=0)\n",
    "    print(f\"{i}번 문제 점수 : accuracy : {acc / len(problem.user_answers)}, f1-score : {f1 / len(problem.user_answers)}\")\n",
    "    total_acc += acc / len(problem.user_answers)\n",
    "    total_f1 += f1 / len(problem.user_answers)\n",
    "print(f\"전체 문제 평균 accuracy : {total_acc / len(dataset)}, f1-score : {total_f1 / len(dataset)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "problem_id = random.choice(list(dataset.keys()))\n",
    "split_answer = random.choice(dataset[problem_id].user_answers).split(' ')\n",
    "word_concat_size = 3\n",
    "tokenized_answer = []\n",
    "for k in range(len(split_answer) - word_concat_size):\n",
    "    tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "print(tokenized_answer[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 유사도 검사 -> Tokenizer 방식에서 띄어쓰기 단위로 2개씩 묶어서 window 방식으로 비교 + 키워드 변형 -> 윈도우 사이즈 3으로 늘림, threshold 0.35로 낮춤\n",
    "threshold = 0.35\n",
    "total_acc, total_f1 = 0, 0\n",
    "word_concat_size = 3\n",
    "\n",
    "for i, key in enumerate(dataset):\n",
    "    acc, f1 = 0, 0\n",
    "    problem = dataset[key]\n",
    "    for j, (user_answer, ground_truth) in enumerate(tqdm(zip(problem.user_answers, problem.ground_truths))):\n",
    "        split_answer = user_answer.split(' ')\n",
    "        tokenized_answer = []\n",
    "        for k in range(len(split_answer) - word_concat_size + 1):\n",
    "            tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "        if len(split_answer) < word_concat_size:\n",
    "            tokenized_answer.append(' '.join(split_answer))\n",
    "        tokenized_answer_embedding = model.encode(tokenized_answer)\n",
    "        similarity_scores = cosine_similarity(problem.keywords_embedding, tokenized_answer_embedding)\n",
    "        predicts = [1 if score.max() > threshold else 0 for idx, score in enumerate(similarity_scores)]\n",
    "\n",
    "        acc += accuracy_score(ground_truth, predicts)\n",
    "        f1 += f1_score(ground_truth, predicts, zero_division=1)\n",
    "    print(f\"{i}번 문제 점수 : accuracy : {acc / len(problem.user_answers)}, f1-score : {f1 / len(problem.user_answers)}\")\n",
    "    total_acc += acc / len(problem.user_answers)\n",
    "    total_f1 += f1 / len(problem.user_answers)\n",
    "print(f\"전체 문제 평균 accuracy : {total_acc / len(dataset)}, f1-score : {total_f1 / len(dataset)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 왜 5번문제만 f1 score가 저 모양일까?\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "problem = dataset['recio3s0A77i0kkEn']\n",
    "\n",
    "print(problem.subject)\n",
    "print(f\"정답 키워드 : {problem.keywords}\")\n",
    "keyword_count = {}\n",
    "for answer, keywords in zip(problem.user_answers, problem.user_correct_keywords):\n",
    "    for keyword in keywords:\n",
    "        if keyword not in keyword_count:\n",
    "            keyword_count[keyword] = 1\n",
    "        else:\n",
    "            keyword_count[keyword] += 1\n",
    "for keyword in keyword_count:\n",
    "    keyword_count[keyword] = f\"{int(keyword_count[keyword] / len(problem.user_answers) * 100)}%\"\n",
    "print(keyword_count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "#### 자세히 보면 키워드가 너무 어려웠는지 정답률이 엉망이다.\n",
    "#### 실제 정답에 1이 없다면 f1 score는 무조건 0이다.\n",
    "#### 이는 키워드가 적절치 않았는지를 고려해봐야 할 듯 하다.\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 이제 실제로 모델이 어떻게 예측하고 있는지 눈으로 확인해보자!!\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 유사도 검사 -> Tokenizer 방식에서 띄어쓰기 단위로 2개씩 묶어서 window 방식으로 비교 + 키워드 변형 -> 윈도우 사이즈 3으로 늘림\n",
    "threshold = 0.35\n",
    "total_acc, total_f1 = 0, 0\n",
    "word_concat_size = 3\n",
    "\n",
    "for i, key in enumerate(dataset):\n",
    "    acc, f1 = 0, 0\n",
    "    problem = dataset[key]\n",
    "    for j, (user_answer, ground_truth) in enumerate(tqdm(zip(problem.user_answers, problem.ground_truths))):\n",
    "        if j == 10:\n",
    "            break\n",
    "        split_answer = user_answer.split(' ')\n",
    "        tokenized_answer = []\n",
    "        for k in range(len(split_answer) - word_concat_size + 1):\n",
    "            tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "        if len(split_answer) < word_concat_size:\n",
    "            tokenized_answer.append(' '.join(split_answer))\n",
    "        tokenized_answer_embedding = model.encode(tokenized_answer)\n",
    "        similarity_scores = cosine_similarity(problem.keywords_embedding, tokenized_answer_embedding)\n",
    "        predicts = []\n",
    "        for z, idx in enumerate(similarity_scores.argmax(axis=1)):\n",
    "            if threshold < similarity_scores[z][idx]:\n",
    "                print(f\"keyword          : {problem.keywords[z]}\")\n",
    "                print(f\"detected keyword : {tokenized_answer[idx]}\")\n",
    "                predicts.append(1)\n",
    "            else:\n",
    "                predicts.append(0)\n",
    "        print(f\"실제 정답 : {problem.user_correct_keywords[j]}\")\n",
    "        acc += accuracy_score(ground_truth, predicts)\n",
    "        f1 += f1_score(ground_truth, predicts, zero_division=1)\n",
    "    print(f\"{i}번 문제 점수 : accuracy : {acc / len(problem.user_answers)}, f1-score : {f1 / len(problem.user_answers)}\")\n",
    "    total_acc += acc / len(problem.user_answers)\n",
    "    total_f1 += f1 / len(problem.user_answers)\n",
    "print(f\"전체 문제 평균 accuracy : {total_acc / len(dataset)}, f1-score : {total_f1 / len(dataset)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 처음에 tokenized 되면서 영어들이 무시되었기 때문에 영어로 구성된 키워드들을 한글로 변형 시켰지만 스윽 보니 이제 영어도 잘 찾네요!!\n",
    "### 그럼 다시 이전 키워드들로 변경해서 성능을 비교해봅시다\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(dataset['recXfKthnQLwWETgb'].keywords)\n",
    "dataset['recXfKthnQLwWETgb'].keywords[0] = \"Lifecycle\"  # 만료 -> Lifecycle\n",
    "print(dataset['recXfKthnQLwWETgb'].keywords)\n",
    "dataset['recXfKthnQLwWETgb'].keywords_embedding = model.encode(dataset['recXfKthnQLwWETgb'].keywords)  # 1번\n",
    "print(dataset['recUcGjT9Xkb7N5pu'].keywords)\n",
    "dataset['recUcGjT9Xkb7N5pu'].keywords[2] = \"삭제(POP)\"  # 삭제 -> 삭제(POP)\n",
    "dataset['recUcGjT9Xkb7N5pu'].keywords[3] = \"삽입(PUSH)\"  # 삽입 -> 삽입(PUSH)\n",
    "print(dataset['recUcGjT9Xkb7N5pu'].keywords)\n",
    "dataset['recUcGjT9Xkb7N5pu'].keywords_embedding = model.encode(dataset['recUcGjT9Xkb7N5pu'].keywords)  # 10번\n",
    "print(dataset['rec1QvAvB4CMami3p'].keywords)\n",
    "dataset['rec1QvAvB4CMami3p'].keywords[2] = 'Stack'  # 스택 -> Stack\n",
    "dataset['rec1QvAvB4CMami3p'].keywords_embedding = model.encode(dataset['rec1QvAvB4CMami3p'].keywords)  # 7번\n",
    "print(dataset['rec1QvAvB4CMami3p'].keywords)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 유사도 검사 -> Tokenizer 방식에서 띄어쓰기 단위로 3개씩 묶어서 window 방식으로 비교 + 키워드 변형 -> 윈도우 사이즈 3으로 늘림\n",
    "threshold = 0.35\n",
    "total_acc, total_f1 = 0, 0\n",
    "word_concat_size = 3\n",
    "\n",
    "for i, key in enumerate(dataset):\n",
    "    acc, f1 = 0, 0\n",
    "    problem = dataset[key]\n",
    "    for j, (user_answer, ground_truth) in enumerate(tqdm(zip(problem.user_answers, problem.ground_truths))):\n",
    "        split_answer = user_answer.split(' ')\n",
    "        tokenized_answer = []\n",
    "        for k in range(len(split_answer) - word_concat_size + 1):\n",
    "            tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "        if len(split_answer) < word_concat_size:\n",
    "            tokenized_answer.append(' '.join(split_answer))\n",
    "        tokenized_answer_embedding = model.encode(tokenized_answer)\n",
    "        similarity_scores = cosine_similarity(problem.keywords_embedding, tokenized_answer_embedding)\n",
    "        predicts = [1 if score.max() > threshold else 0 for idx, score in enumerate(similarity_scores)]\n",
    "\n",
    "        acc += accuracy_score(ground_truth, predicts)\n",
    "        f1 += f1_score(ground_truth, predicts, zero_division=1)\n",
    "    print(f\"{i}번 문제 점수 : accuracy : {acc / len(problem.user_answers)}, f1-score : {f1 / len(problem.user_answers)}\")\n",
    "    total_acc += acc / len(problem.user_answers)\n",
    "    total_f1 += f1 / len(problem.user_answers)\n",
    "print(f\"전체 문제 평균 accuracy : {total_acc / len(dataset)}, f1-score : {total_f1 / len(dataset)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 1번 문제 : 만료 -> Lifecycle [성능 소폭 하락]\n",
    "### 7번 문제 : 삽입 -> 스택 -> Stack [성능 상승]\n",
    "### 10번 문제 : 삭제 -> 삭제(POP), 삽입 -> 삽입(PUSH) [성능 하락]\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 이번에는 여러개의 키워드라면 쉼표로 구분해서 넣어줘보자\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(dataset['recXfKthnQLwWETgb'].keywords)\n",
    "dataset['recXfKthnQLwWETgb'].keywords[0] = \"만료, Lifecycle\"\n",
    "print(dataset['recXfKthnQLwWETgb'].keywords)\n",
    "dataset['recXfKthnQLwWETgb'].keywords_embedding = model.encode(dataset['recXfKthnQLwWETgb'].keywords)  # 1번\n",
    "print(dataset['recUcGjT9Xkb7N5pu'].keywords)\n",
    "dataset['recUcGjT9Xkb7N5pu'].keywords[2] = \"삭제, POP\"  # 삭제 -> 삭제(POP)\n",
    "dataset['recUcGjT9Xkb7N5pu'].keywords[3] = \"삽입, PUSH\"  # 삽입 -> 삽입(PUSH)\n",
    "print(dataset['recUcGjT9Xkb7N5pu'].keywords)\n",
    "dataset['recUcGjT9Xkb7N5pu'].keywords_embedding = model.encode(dataset['recUcGjT9Xkb7N5pu'].keywords)  # 10번\n",
    "print(dataset['rec1QvAvB4CMami3p'].keywords)\n",
    "dataset['rec1QvAvB4CMami3p'].keywords[2] = 'Stack'  # 스택 -> Stack\n",
    "dataset['rec1QvAvB4CMami3p'].keywords_embedding = model.encode(dataset['rec1QvAvB4CMami3p'].keywords)  # 7번\n",
    "print(dataset['rec1QvAvB4CMami3p'].keywords)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 유사도 검사 -> Tokenizer 방식에서 띄어쓰기 단위로 2개씩 묶어서 window 방식으로 비교 + 키워드 변형 -> 윈도우 사이즈 3으로 늘림\n",
    "threshold = 0.35\n",
    "total_acc, total_f1 = 0, 0\n",
    "word_concat_size = 3\n",
    "\n",
    "for i, key in enumerate(dataset):\n",
    "    acc, f1 = 0, 0\n",
    "    problem = dataset[key]\n",
    "    for j, (user_answer, ground_truth) in enumerate(tqdm(zip(problem.user_answers, problem.ground_truths))):\n",
    "        split_answer = user_answer.split(' ')\n",
    "        tokenized_answer = []\n",
    "        for k in range(len(split_answer) - word_concat_size + 1):\n",
    "            tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "        if len(split_answer) < word_concat_size:\n",
    "            tokenized_answer.append(' '.join(split_answer))\n",
    "        tokenized_answer_embedding = model.encode(tokenized_answer)\n",
    "        similarity_scores = cosine_similarity(problem.keywords_embedding, tokenized_answer_embedding)\n",
    "        predicts = [1 if score.max() > threshold else 0 for idx, score in enumerate(similarity_scores)]\n",
    "\n",
    "        acc += accuracy_score(ground_truth, predicts)\n",
    "        f1 += f1_score(ground_truth, predicts, zero_division=1)\n",
    "    print(f\"{i}번 문제 점수 : accuracy : {acc / len(problem.user_answers)}, f1-score : {f1 / len(problem.user_answers)}\")\n",
    "    total_acc += acc / len(problem.user_answers)\n",
    "    total_f1 += f1 / len(problem.user_answers)\n",
    "print(f\"전체 문제 평균 accuracy : {total_acc / len(dataset)}, f1-score : {total_f1 / len(dataset)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 꽤 유의미한 결과다!!\n",
    "### 마지막으로 word concat size를 2로 줄여서 두개의 단어씩만 비교해보자\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "problem_id = random.choice(list(dataset.keys()))\n",
    "\n",
    "split_answer = random.choice(dataset[problem_id].user_answers).split(' ')\n",
    "word_concat_size = 2\n",
    "tokenized_answer = []\n",
    "for k in range(len(split_answer) - word_concat_size):\n",
    "    tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "print(tokenized_answer[:3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 유사도 검사 -> Tokenizer 방식에서 띄어쓰기 단위로 2개씩 묶어서 window 방식으로 비교 + 키워드 변형 -> 윈도우 사이즈 3으로 늘림\n",
    "threshold = 0.35\n",
    "total_acc, total_f1 = 0, 0\n",
    "word_concat_size = 2\n",
    "\n",
    "for i, key in enumerate(dataset):\n",
    "    acc, f1 = 0, 0\n",
    "    problem = dataset[key]\n",
    "    for j, (user_answer, ground_truth) in enumerate(tqdm(zip(problem.user_answers, problem.ground_truths))):\n",
    "        split_answer = user_answer.split(' ')\n",
    "        tokenized_answer = []\n",
    "        for k in range(len(split_answer) - word_concat_size + 1):\n",
    "            tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "        if len(split_answer) < word_concat_size:\n",
    "            tokenized_answer.append(' '.join(split_answer))\n",
    "        tokenized_answer_embedding = model.encode(tokenized_answer)\n",
    "        similarity_scores = cosine_similarity(problem.keywords_embedding, tokenized_answer_embedding)\n",
    "        predicts = [1 if score.max() > threshold else 0 for idx, score in enumerate(similarity_scores)]\n",
    "\n",
    "        acc += accuracy_score(ground_truth, predicts)\n",
    "        f1 += f1_score(ground_truth, predicts, zero_division=1)\n",
    "    print(f\"{i}번 문제 점수 : accuracy : {acc / len(problem.user_answers)}, f1-score : {f1 / len(problem.user_answers)}\")\n",
    "    total_acc += acc / len(problem.user_answers)\n",
    "    total_f1 += f1 / len(problem.user_answers)\n",
    "print(f\"전체 문제 평균 accuracy : {total_acc / len(dataset)}, f1-score : {total_f1 / len(dataset)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### 전체 정확도는 조금 떨어졌지만 분명 2개씩 봤을 때 성능이 더 좋은 문제들이 있다.\n",
    "### 확실하게 하려면 다음부터는 2개씩 보는 것과 3개씩 보는 것, 그리고 쉼표로 여러 키워드 후보를 비교해보는게 아닌 각각 여러개 비교하기까지 하면 성능이 꽤나 좋아질 것 같다.\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 마지막으로 윈도우 사이즈를 2개와 3개를 합쳐서 해보자!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "problem_id = random.choice(list(dataset.keys()))\n",
    "split_answer = random.choice(dataset[problem_id].user_answers).split(' ')\n",
    "word_concat_size = 2\n",
    "for k in range(len(split_answer) - word_concat_size + 1):\n",
    "    tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "word_concat_size = 3\n",
    "for k in range(len(split_answer) - word_concat_size + 1):\n",
    "    tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "print(tokenized_answer[:3])\n",
    "print(tokenized_answer[-3:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 유사도 검사 -> Tokenizer 방식에서 띄어쓰기 단위로 2개씩 묶어서 window 방식으로 비교 + 키워드 변형 -> 윈도우 사이즈 3으로 늘림\n",
    "threshold = 0.35\n",
    "total_acc, total_f1 = 0, 0\n",
    "\n",
    "for i, key in enumerate(dataset):\n",
    "    acc, f1 = 0, 0\n",
    "    problem = dataset[key]\n",
    "    for j, (user_answer, ground_truth) in enumerate(tqdm(zip(problem.user_answers, problem.ground_truths))):\n",
    "        split_answer = user_answer.split(' ')\n",
    "        tokenized_answer = []\n",
    "        word_concat_size = 2\n",
    "        for k in range(len(split_answer) - word_concat_size + 1):\n",
    "            tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "        word_concat_size = 3\n",
    "        for k in range(len(split_answer) - word_concat_size + 1):\n",
    "            tokenized_answer.append(' '.join(split_answer[k : k + word_concat_size]))\n",
    "        if not tokenized_answer:\n",
    "            tokenized_answer.append(' '.join(split_answer))\n",
    "        tokenized_answer_embedding = model.encode(tokenized_answer)\n",
    "        similarity_scores = cosine_similarity(problem.keywords_embedding, tokenized_answer_embedding)\n",
    "        predicts = [1 if score.max() > threshold else 0 for idx, score in enumerate(similarity_scores)]\n",
    "\n",
    "        acc += accuracy_score(ground_truth, predicts)\n",
    "        f1 += f1_score(ground_truth, predicts, zero_division=1)\n",
    "    print(f\"{i}번 문제 점수 : accuracy : {acc / len(problem.user_answers)}, f1-score : {f1 / len(problem.user_answers)}\")\n",
    "    total_acc += acc / len(problem.user_answers)\n",
    "    total_f1 += f1 / len(problem.user_answers)\n",
    "print(f\"전체 문제 평균 accuracy : {total_acc / len(dataset)}, f1-score : {total_f1 / len(dataset)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 하지만 이런 방식은 너무 느리다. GPU를 사용한다해도 이는 감당이 안될 수 있다.\n",
    "### 그리고 정확도도 그리 좋아지지 않았다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.corpus.stopwords.words('english')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('korean')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(stopwords)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No such file or directory: '/Users/minjaewon/nltk_data/corpora/stopwords/korean'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Input \u001B[0;32mIn [34]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m stopwords \u001B[38;5;241m=\u001B[39m \u001B[43mnltk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcorpus\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstopwords\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwords\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mkorean\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ai-resesarch/lib/python3.8/site-packages/nltk/corpus/reader/wordlist.py:21\u001B[0m, in \u001B[0;36mWordListCorpusReader.words\u001B[0;34m(self, fileids, ignore_lines_startswith)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwords\u001B[39m(\u001B[38;5;28mself\u001B[39m, fileids\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, ignore_lines_startswith\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m     20\u001B[0m         line\n\u001B[0;32m---> 21\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m line_tokenize(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfileids\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     22\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m line\u001B[38;5;241m.\u001B[39mstartswith(ignore_lines_startswith)\n\u001B[1;32m     23\u001B[0m     ]\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ai-resesarch/lib/python3.8/site-packages/nltk/corpus/reader/api.py:218\u001B[0m, in \u001B[0;36mCorpusReader.raw\u001B[0;34m(self, fileids)\u001B[0m\n\u001B[1;32m    216\u001B[0m contents \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m fileids:\n\u001B[0;32m--> 218\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m fp:\n\u001B[1;32m    219\u001B[0m         contents\u001B[38;5;241m.\u001B[39mappend(fp\u001B[38;5;241m.\u001B[39mread())\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m concat(contents)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ai-resesarch/lib/python3.8/site-packages/nltk/corpus/reader/api.py:231\u001B[0m, in \u001B[0;36mCorpusReader.open\u001B[0;34m(self, file)\u001B[0m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    224\u001B[0m \u001B[38;5;124;03mReturn an open stream that can be used to read the given file.\u001B[39;00m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;124;03mIf the file's encoding is not None, then the stream will\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    228\u001B[0m \u001B[38;5;124;03m:param file: The file identifier of the file to read.\u001B[39;00m\n\u001B[1;32m    229\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    230\u001B[0m encoding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoding(file)\n\u001B[0;32m--> 231\u001B[0m stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_root\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mopen(encoding)\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m stream\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ai-resesarch/lib/python3.8/site-packages/nltk/data.py:334\u001B[0m, in \u001B[0;36mFileSystemPathPointer.join\u001B[0;34m(self, fileid)\u001B[0m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mjoin\u001B[39m(\u001B[38;5;28mself\u001B[39m, fileid):\n\u001B[1;32m    333\u001B[0m     _path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_path, fileid)\n\u001B[0;32m--> 334\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mFileSystemPathPointer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ai-resesarch/lib/python3.8/site-packages/nltk/compat.py:41\u001B[0m, in \u001B[0;36mpy3_data.<locals>._decorator\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_decorator\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     40\u001B[0m     args \u001B[38;5;241m=\u001B[39m (args[\u001B[38;5;241m0\u001B[39m], add_py3_data(args[\u001B[38;5;241m1\u001B[39m])) \u001B[38;5;241m+\u001B[39m args[\u001B[38;5;241m2\u001B[39m:]\n\u001B[0;32m---> 41\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minit_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/ai-resesarch/lib/python3.8/site-packages/nltk/data.py:312\u001B[0m, in \u001B[0;36mFileSystemPathPointer.__init__\u001B[0;34m(self, _path)\u001B[0m\n\u001B[1;32m    310\u001B[0m _path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mabspath(_path)\n\u001B[1;32m    311\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(_path):\n\u001B[0;32m--> 312\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo such file or directory: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m _path)\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_path \u001B[38;5;241m=\u001B[39m _path\n",
      "\u001B[0;31mOSError\u001B[0m: No such file or directory: '/Users/minjaewon/nltk_data/corpora/stopwords/korean'"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('korean')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "179"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
